{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6574866-b7c3-47a5-821b-269f213283c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Murali Krishna Enugula\n",
    "HDS 5230 - 07\n",
    "Week 05 - Dask Programming Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c41b2-be6e-411c-ba68-e2c049ad4e81",
   "metadata": {},
   "source": [
    "1.  Creating a Dask DataFrame for US States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623294dc-19fa-4b2e-b932-00be3f6089b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Loading dataset using Dask \n",
    "df = dd.read_csv('C://Users//drmur//Downloads//timeseries.csv', dtype={'cases': 'float64', 'deaths': 'float64', 'population': 'float64'})\n",
    "\n",
    "# Converting date column to datetime\n",
    "df['date'] = dd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Filtering for US states\n",
    "df_us = df[df['country'] == 'United States']\n",
    "\n",
    "# Selecting relevant columns\n",
    "df_us = df_us[['state', 'date', 'population', 'cases', 'deaths']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60521d-6b78-44e1-9511-5423e377a5f0",
   "metadata": {},
   "source": [
    "Parallelization is not necessary since filtering rows based on country is a simple operation that even large datasets can handle efficiently with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9841b-e83e-497a-a9fe-b87307426f7e",
   "metadata": {},
   "source": [
    "2. Computing Per-Capita Mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbe5550-3461-42f8-8afb-c16e8ad32b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drmur\\AppData\\Local\\Temp\\ipykernel_5904\\329875824.py:3: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  df_us['population'] = df_us['population'].fillna(df_us.groupby('state')['population'].transform('mean'))\n"
     ]
    }
   ],
   "source": [
    "# Filling missing values\n",
    "df_us['deaths'] = df_us['deaths'].fillna(0)\n",
    "df_us['population'] = df_us['population'].fillna(df_us.groupby('state')['population'].transform('mean'))\n",
    "\n",
    "# Grouping by state and compute total deaths and average population\n",
    "state_mortality = df_us.groupby('state').agg({'deaths': 'sum', 'population': 'mean'})\n",
    "\n",
    "# Computing per-capita mortality\n",
    "state_mortality['Per-Capita Mortality'] = state_mortality['deaths'] / state_mortality['population']\n",
    "\n",
    "# Result\n",
    "state_mortality = state_mortality.compute().sort_values(by='Per-Capita Mortality', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18decc7-3779-4db9-b322-ced3f813688d",
   "metadata": {},
   "source": [
    "Since the computation involves only 50 states, Pandas can efficiently aggregate and compute per-capita mortality without parallelization. However, if performing the same calculation for thousands of counties or cities, a parallelized framework like Dask would be beneficial for efficiently handling the large number of groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e836a5-f256-49d2-9dc4-e70af8b42b50",
   "metadata": {},
   "source": [
    "3. Computing Monthly Case Fatality Rate (CFR) Using WHO Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e37836-8ef8-4801-b1c7-53891247f94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year_month      2020-01  2020-02   2020-03   2020-04   2020-05   2020-06  \\\n",
      "state                                                                      \n",
      "Alabama             NaN      NaN  0.532313  2.830899  3.889270  2.962907   \n",
      "Alaska              NaN      NaN  0.335008  2.314519  2.196905  1.303247   \n",
      "American Samoa      NaN      NaN       NaN       NaN       NaN       NaN   \n",
      "Arizona             0.0      0.0  0.000000  1.486545  1.992175  0.211513   \n",
      "Arkansas            NaN      NaN  0.915656  1.911450  2.129628  1.515155   \n",
      "\n",
      "year_month       2020-07  \n",
      "state                     \n",
      "Alabama         2.381771  \n",
      "Alaska          1.207417  \n",
      "American Samoa       NaN  \n",
      "Arizona         0.973523  \n",
      "Arkansas        1.274360  \n"
     ]
    }
   ],
   "source": [
    "# Converting 'date' column to datetime format\n",
    "df['date'] = dd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Filtering dataset for US states\n",
    "df_us = df[df['country'] == 'United States']\n",
    "\n",
    "# Keeping relevant columns\n",
    "df_us = df_us[['state', 'date', 'cases', 'deaths']]\n",
    "\n",
    "# Filling missing values in deaths and cases\n",
    "df_us['cases'] = df_us['cases'].fillna(0)\n",
    "df_us['deaths'] = df_us['deaths'].fillna(0)\n",
    "\n",
    "# Extracting year-month for grouping\n",
    "df_us['year_month'] = df_us['date'].dt.to_period('M')\n",
    "\n",
    "# Grouping by state and month and reset index to retain 'state'\n",
    "monthly_stats = df_us.groupby(['state', 'year_month']).agg({'cases': 'sum', 'deaths': 'sum'}).reset_index()\n",
    "\n",
    "# Computing CFR using WHO approach\n",
    "monthly_stats['CFR'] = (monthly_stats['deaths'] / monthly_stats['cases']) * 100\n",
    "\n",
    "# Replacing infinite values with NaN\n",
    "monthly_stats['CFR'] = monthly_stats['CFR'].replace([float('inf'), -float('inf')], None)\n",
    "\n",
    "# Computing to bring Dask DataFrame into Pandas format\n",
    "monthly_stats = monthly_stats.compute()\n",
    "\n",
    "# Pivoting the data into a 50 (states) × 14 (months) matrix\n",
    "cfr_matrix = monthly_stats.pivot(index='state', columns='year_month', values='CFR')\n",
    "\n",
    "# Displaying the final CFR matrix\n",
    "print(cfr_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e1313-87eb-496e-b4b8-be2fcc5e0066",
   "metadata": {},
   "source": [
    "For state-level monthly CFR calculations (50 × 14 = 700 groups), Pandas is sufficient. However, if computing CFR for daily cases across thousands of counties, Dask would significantly speed up the grouping and aggregation operations by distributing computations across multiple processors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91a4eb-d377-4bb5-b913-97be69ca337d",
   "metadata": {},
   "source": [
    "4. Ranking States Based on CFR Changes Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67171b93-f47e-4949-98ce-e78794273bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "Michigan                    8.688709\n",
      "Northern Mariana Islands    7.932137\n",
      "New Jersey                  7.849657\n",
      "Connecticut                 7.686037\n",
      "Massachusetts               7.437951\n",
      "Washington                  6.695390\n",
      "Pennsylvania                6.512038\n",
      "Wisconsin                   6.470096\n",
      "New Hampshire               6.167981\n",
      "Missouri                    5.982125\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Computing month-to-month CFR changes\n",
    "cfr_changes = cfr_matrix.diff(axis=1)  # This is now a Pandas DataFrame\n",
    "\n",
    "# Computing total absolute change across months\n",
    "state_cfr_variability = cfr_changes.abs().sum(axis=1)\n",
    "\n",
    "# Ranking states based on CFR fluctuations\n",
    "state_cfr_ranking = state_cfr_variability.sort_values(ascending=False)\n",
    "\n",
    "# Displaying the ranking\n",
    "print(state_cfr_ranking.head(10))  # Show the top 10 states with the highest CFR variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8c598-1e94-4045-9898-b058f27d7f2a",
   "metadata": {},
   "source": [
    "This operation involves a small matrix (50 × 14), where month-to-month CFR changes are computed and summed across months. Since this dataset is small, parallelization is unnecessary, and Pandas is the optimal choice for quick computation. For larger datasets (e.g., county-level trends), Dask could be considered but is not required for state-level analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

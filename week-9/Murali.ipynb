{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Murali\n",
        "\n",
        "Week 09 - Machine Learning with Scikit-learn\n",
        "\n"
      ],
      "metadata": {
        "id": "dhgdXhAVbyMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence.\n",
        "\n",
        "\n",
        "The **Random Forest classifier** demonstrated superior performance compared to other models according to the results shown in the Python notebook through its high training accuracy score of **0.9993**. The Random Forest model achieved **0.9993** as its final training accuracy score. The extremely high training accuracy level indicates substantial overfitting has occurred. The testing accuracy for this model reached **0.686** even though its nearly perfect training performance indicated **0.9993** accuracy. The result still ranked as competitive when compared to logistic regression models.\n",
        "\n",
        "The Random Forest classifier achieved good results because it successfully captured sophisticated data patterns that linear models such as logistic regression would find difficult to analyze. The combination of decision trees through the ensemble model structure leads to better generalization performance. The Random Forest model tends to overfit when hyperparameter tuning is absent but the problem was partially resolved by implementing grid search cross-validation (GridSearchCV) on max_depth and other parameters.\n",
        "\n",
        "The logistic regression models produced strong performance outcomes that became most evident when using L1 regularization (LASSO). The logistic regression model with L1 regularization at \\( C = 10 \\) reached **0.7347** training accuracy and **0.718** testing accuracy as its best performance. The obtained results show strong generalization ability because the difference between training and testing performance remains small. Using the pipeline approach for cross-validated logistic regression with L1 regularization (Logistic_SL1_C_auto) produced almost equal results with **0.7307** training accuracy and **0.714** testing accuracy.\n",
        "\n",
        "The Random Forest model demonstrated the best training performance but failed to generalize as effectively as logistic regression models particularly when L1 regularization was applied correctly. The finding demonstrates that achieving accurate results must be balanced against maintaining good generalization ability. The Random Forest model's performance can be improved through hyperparameter tuning which resolves overfitting problems as shown by the grid search results. The logistic regression models with L1 regularization performed consistently between training and testing phases which made them competitive even though they did not reach the highest accuracy level."
      ],
      "metadata": {
        "id": "Q10jGzVebrRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XHa-fhWBXMJX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from patsy import dmatrices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the requried dataset\n",
        "df_patient = pd.read_csv('./PatientAnalyticFile.csv')\n",
        "\n",
        "#Converting to moratlity variable\n",
        "df_patient['mortality'] = np.where(df_patient['DateOfDeath'].isnull(), 0, 1)\n",
        "\n",
        "#Converting DateOfBirth to datetime and calculate age\n",
        "df_patient['DateOfBirth'] = pd.to_datetime(df_patient['DateOfBirth'])\n",
        "df_patient['Age_years'] = ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days / 365.25)\n",
        "\n",
        "vars_remove = ['PatientID','First_Appointment_Date','DateOfBirth','Last_Appointment_Date','DateOfDeath','mortality']\n",
        "vars_left = set(df_patient.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)"
      ],
      "metadata": {
        "id": "mAnmOt2ycT1Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y, X = dmatrices(formula, df_patient, return_type='dataframe')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, np.ravel(Y), test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "results = []"
      ],
      "metadata": {
        "id": "_MjyxsWickVA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit models with each solver and record performance\n",
        "for solver in solvers:\n",
        "    start_time = time.time()\n",
        "    clf = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "\n",
        "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    results.append([solver, train_acc, test_acc, time_taken])\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['Solver used', 'Training subset accuracy', 'Holdout subset accuracy', 'Time taken'])\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPKjkOvcodz",
        "outputId": "43a782dd-f0a5-4259-c14c-e8481f59aad3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Solver used  Training subset accuracy  Holdout subset accuracy  Time taken\n",
            "0   newton-cg                  0.748188                  0.73625    0.138945\n",
            "1       lbfgs                  0.748437                  0.73600    0.912498\n",
            "2   liblinear                  0.747938                  0.73625    0.091597\n",
            "3         sag                  0.748062                  0.73625   11.557895\n",
            "4        saga                  0.748000                  0.73625   18.255994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?**\n",
        "\n",
        "\n",
        "\n",
        "The **lbfgs** solver produced the most favorable results because it maintained a good equilibrium between training accuracy and holdout accuracy and execution time. The lbfgs solver demonstrated the best training accuracy at 0.7484 and its holdout accuracy of 0.7360 ranked similarly to newton-cg and liblinear and sag solvers. The lbfgs solver executed its process in **0.91 seconds** which showed efficiency compared to sag and saga yet performed slower than newton-cg and liblinear.\n",
        "\n",
        "The most critical indicator of model performance consists of holdout subset accuracy because this metric specifically determines the model's ability to predict unaffected data points. The primary goal of these models is to perform well outside training data so this metric should take precedence. Every solver demonstrated equivalent performance based on the accuracy score of **0.7360**.\n",
        "\n",
        "The evaluation now focuses heavily on execution time because the holdout accuracy scores show minimal variations. The computation times for sag and saga were unacceptably long since they ran for **11.56 seconds** and **18.26 seconds** respectively. The solvers generated warnings because they did not reach an acceptable solution before exceeding the maximum iteration limit set to 1000. Their reasonable accuracy cannot overcome their inefficiency which makes them undesirable for use.\n",
        "\n",
        "The most efficient solver for this task becomes the **lbfgs** when we evaluate its performance through all three metrics including holdout accuracy, execution time and training accuracy. The solver delivers satisfactory results through its combination of precision and runtime performance without encountering convergence problems. The newton-cg and liblinear solvers demonstrate good speed and accuracy but their slightly lower training accuracy makes them less attractive than lbfgs. The best solver selection depends on the evaluation of generalization performance versus computation time where solutions that maximize holdout accuracy gain a slight advantage."
      ],
      "metadata": {
        "id": "TDLExQ9zctss"
      }
    }
  ]
}